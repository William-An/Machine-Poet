{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange\n",
    "from tqdm import tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to Vector Representation\n",
    "embedding_feature_columns = hub.text_embedding_column(key = \"sentence\", module_spec = \"https://tfhub.dev/google/Wiki-words-500-with-normalization/1\", trainable=False)\n",
    "# emb = hub.Module(\"https://tfhub.dev/google/Wiki-words-500-with-normalization/1\")\n",
    "\n",
    "# NNML By Google\n",
    "# URL = https://tfhub.dev/google/nnlm-en-dim50/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def dataset_input_fn():\n",
    "    #def _parsefn(sentence):   \n",
    "    filenames = [\"poems/shakespeare/sonnets.txt\"]\n",
    "    # TODO Use skip and filter methods to preprocess data rather than manually do it\n",
    "    # TODO Use Dataset.map method to map '\\n' to 'xxxnewlinexxx'\n",
    "    dataset = tf.data.TextLineDataset(filenames)\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    #dataset = dataset.batch(4)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_poem = iterator.get_next()\n",
    "    return {\"sentence\":next_poem}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(emb([dataset_input_fn()[\"words\"]]))\n",
    "with tf.Graph().as_default():\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim50/1\")\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        print(sess.run(embed([\" \"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(emb([dataset_input_fn()[\"words\"]]))\n",
    "with tf.Graph().as_default():\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/1\")\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        print(sess.run(tf.string_split(dataset_input_fn()[\"sentence\"])))\n",
    "        # print(sess.run(embed([dataset_input_fn()[\"sentence\"]])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to Vector Representation\n",
    "embedding_feature_columns = hub.text_embedding_column(key = \"sentence\", module_spec = \"https://tfhub.dev/google/nnlm-en-dim50/1\", trainable=False)\n",
    "# emb = hub.Module(\"https://tfhub.dev/google/Wiki-words-500-with-normalization/1\")\n",
    "\n",
    "# NNML By Google\n",
    "# URL = https://tfhub.dev/google/nnlm-en-dim50/1\n",
    "def dataset_input_fn():\n",
    "    def _insertSpace(sentence):\n",
    "        sentence = sentence.decode()\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'([\\W\\d])', r' \\1 ', sentence)\n",
    "        return sentence\n",
    "    # Use tf.string_split if want to split string\n",
    "    # def _split(sentence):\n",
    "    #    return sentence.split()\n",
    "    def _getLabel(sentence):\n",
    "        splited = tf.string_split(sentence).values\n",
    "        ids = table.lookup(splited)\n",
    "        return {\"sentence\":sentence}, ids\n",
    "    filenames = [\"poems/shakespeare/sonnets.txt\"]\n",
    "    # TODO Use skip and filter methods to preprocess data rather than manually do it\n",
    "    # TODO Use Dataset.map method to map '\\n' to 'xxxnewlinexxx'\n",
    "    dataset = tf.data.TextLineDataset(filenames)\n",
    "    dataset = dataset.map(lambda sentence: tf.py_func(_insertSpace, [sentence], tf.string))\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(4)\n",
    "    dataset = dataset.map(_getLabel)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_poem = iterator.get_next()\n",
    "    return next_poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_fn(features, labels, params):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string2idtable = tf.contrib.lookup.index_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\", num_oov_buckets=0)\n",
    "ids = table.lookup(tf.constant([\"!\", \"lake\", \"and\", \"palmer\"]))\n",
    "id2stringtabel = tf.contrib.lookup.index_to_string_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\")\n",
    "string = id2stringtabel.lookup(tf.constant([1, 3, 5, 2365], dtype=tf.int64))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.tables_initializer().run(session=sess)\n",
    "print(ids.eval(session=sess))\n",
    "print(string.eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _insertSpace(sentence):\n",
    "    sentence = sentence.decode()\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'([\\W\\d])', r' \\1 ', sentence)\n",
    "    return sentence\n",
    "# Use tf.string_split if want to split string\n",
    "# def _split(sentence):\n",
    "#    return sentence.split()\n",
    "def _getLabel(sentence):\n",
    "    splited = tf.string_split([sentence]).values\n",
    "    sentence = splited\n",
    "    # Use \" \" as x_0\n",
    "    sentence = tf.concat([tf.constant([\" \"], dtype=tf.string), sentence[0:-1]], 0)\n",
    "    ids = string2idtable.lookup(splited)\n",
    "    return {\"sentence\":sentence}, ids\n",
    "filenames = [\"poems/shakespeare/sonnets.txt\"]\n",
    "# TODO Use skip and filter methods to preprocess data rather than manually do it\n",
    "# TODO Use Dataset.map method to map '\\n' to 'xxxnewlinexxx'\n",
    "dataset = tf.data.TextLineDataset(filenames)\n",
    "dataset = dataset.map(lambda sentence: tf.py_func(_insertSpace, [sentence], tf.string))\n",
    "dataset = dataset.map(_getLabel)\n",
    "# dataset = dataset.shuffle(buffer_size=10000)\n",
    "# dataset = dataset.batch(4)\n",
    "\n",
    "dataset = dataset.repeat()\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_poem = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated\n",
    "with tf.Graph().as_default():\n",
    "    # Dataset\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim50/1\")\n",
    "    string2idtable = tf.contrib.lookup.index_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\", num_oov_buckets=0)\n",
    "    id2stringtabel = tf.contrib.lookup.index_to_string_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\")\n",
    "    def _insertSpace(sentence):\n",
    "        sentence = sentence.decode()\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'([\\W\\d])', r' \\1 ', sentence)\n",
    "        return sentence\n",
    "    # Use tf.string_split if want to split string\n",
    "    # def _split(sentence):\n",
    "    #    return sentence.split()\n",
    "    def _getLabel(sentence):\n",
    "        splited = tf.string_split([sentence]).values\n",
    "        sentence = splited\n",
    "        # Use \" \" as x_0\n",
    "        sentence = tf.concat([tf.constant([\" \"], dtype=tf.string), sentence[0:-1]], 0)\n",
    "        ids = string2idtable.lookup(splited)\n",
    "        return {\"sentence\":sentence}, ids# {\"sentence\":sentence}, ids\n",
    "    filenames = [\"poems/shakespeare/sonnets.txt\"]\n",
    "    # TODO Use skip and filter methods to preprocess data rather than manually do it\n",
    "    # TODO Use Dataset.map method to map '\\n' to 'xxxnewlinexxx'\n",
    "    dataset = tf.data.TextLineDataset(filenames)\n",
    "    dataset = dataset.map(lambda sentence: tf.py_func(_insertSpace, [sentence], tf.string))\n",
    "    dataset = dataset.map(_getLabel)\n",
    "    # dataset = dataset.shuffle(buffer_size=10000)\n",
    "    # dataset = dataset.batch(4)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_poem = iterator.get_next()\n",
    "    \n",
    "    # Variables\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", shape=[128, 11405], initializer=tf.random_normal_initializer)\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", shape=[1, 11405], initializer=tf.random_normal_initializer)\n",
    "    #lstm.build(inputs_shape=[1, 50])\n",
    "    # IDEA Won't create variables until this instance has been called\n",
    "    lstm = tf.contrib.rnn.LSTMCell(128, state_is_tuple=True, initializer=tf.random_normal_initializer, reuse=tf.AUTO_REUSE, name=\"lstm1\")\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tf.tables_initializer().run(session=sess)\n",
    "        sess.run(iterator.initializer)\n",
    "        sentence, label = next_poem\n",
    "        sentence = sentence[\"sentence\"]\n",
    "        sentence = embed(sentence)\n",
    "        sentence = tf.concat([tf.zeros([1, 50]), sentence[1:]], 0)\n",
    "        label = tf.one_hot(label, 11405)\n",
    "        for _ in range(1):\n",
    "            # Pre for each poem\n",
    "            initial_state = state = tf.zeros([1, lstm.state_size.c]), tf.zeros([1, lstm.state_size.h])\n",
    "            # x, y = sess.run([sentence, label])\n",
    "            x, y = sentence, label\n",
    "            for i in range(10):\n",
    "                # Each timestep\n",
    "                z = tf.reshape(x[i], [1, 50])\n",
    "                output, state = lstm(z, state)\n",
    "                logits = tf.add(tf.matmul(output, softmax_w), softmax_b)\n",
    "                possibility = tf.nn.softmax_cross_entropy_with_logits_v2(labels=label[i], logits=logits)\n",
    "                print(sess.run(possibility))\n",
    "            \n",
    "            \n",
    "            \n",
    "#for _ in range(1):\n",
    "#    poem = sess.run(next_poem)\n",
    "#    print(type(poem[0][\"sentence\"]))\n",
    "#    print(type(poem[1]))\n",
    "# (lambda: next_poem)() # As input_fn? for Estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 - Current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/embeddings/part_0:0 from checkpoint b'/var/folders/qz/dx3zfgtj2lqf70kv8b90s8wr0000gp/T/tfhub_modules/7f07056e3a4c9f125d5bd920ef3883605d8556a8/variables/variables' with embeddings\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x128abfdd8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "INFO:tensorflow:Loss: 17.51368 | local step: 1 | global step: 1 | Output: [b'ft']\n",
      "INFO:tensorflow:Loss: 37.096455 | local step: 101 | global step: 101 | Output: [b'authorized']\n",
      "INFO:tensorflow:Loss: 34.36387 | local step: 51 | global step: 201 | Output: [b'enhancing']\n",
      "INFO:tensorflow:Loss: 42.92499 | local step: 151 | global step: 301 | Output: [b'esteeming']\n",
      "INFO:tensorflow:Loss: 42.02422 | local step: 90 | global step: 401 | Output: [b'prepared']\n",
      "INFO:tensorflow:Loss: 28.092344 | local step: 41 | global step: 501 | Output: [b'friendship']\n",
      "INFO:tensorflow:Loss: 38.09091 | local step: 141 | global step: 601 | Output: [b'desk']\n",
      "INFO:tensorflow:Loss: 27.584349 | local step: 100 | global step: 701 | Output: [b'positive']\n",
      "INFO:tensorflow:Loss: 52.29058 | local step: 53 | global step: 801 | Output: [b'guys']\n",
      "INFO:tensorflow:Loss: 35.780575 | local step: 153 | global step: 901 | Output: [b'sky']\n",
      "INFO:tensorflow:Loss: 40.340714 | local step: 94 | global step: 1001 | Output: [b'oppression']\n",
      "INFO:tensorflow:Loss: 37.486202 | local step: 52 | global step: 1101 | Output: [b'lov']\n",
      "INFO:tensorflow:Loss: 38.283245 | local step: 152 | global step: 1201 | Output: [b'zoloft']\n",
      "INFO:tensorflow:Loss: 41.923946 | local step: 95 | global step: 1301 | Output: [b'longer']\n",
      "INFO:tensorflow:Loss: 25.548458 | local step: 38 | global step: 1401 | Output: [b'couplement']\n",
      "INFO:tensorflow:Loss: 36.630753 | local step: 138 | global step: 1501 | Output: [b'clark']\n",
      "INFO:tensorflow:Loss: 34.076576 | local step: 76 | global step: 1601 | Output: [b'newcastle']\n",
      "INFO:tensorflow:Loss: 23.058214 | local step: 12 | global step: 1701 | Output: [b'soap']\n",
      "INFO:tensorflow:Loss: 44.807148 | local step: 112 | global step: 1801 | Output: [b'indeed']\n",
      "INFO:tensorflow:Loss: 27.273582 | local step: 60 | global step: 1901 | Output: [b'baking']\n",
      "INFO:tensorflow:Loss: 30.36439 | local step: 11 | global step: 2001 | Output: [b'patients']\n",
      "INFO:tensorflow:Loss: 33.397804 | local step: 111 | global step: 2101 | Output: [b'contributed']\n",
      "INFO:tensorflow:Loss: 32.558697 | local step: 61 | global step: 2201 | Output: [b'beautifully']\n",
      "INFO:tensorflow:Loss: 38.70229 | local step: 23 | global step: 2301 | Output: [b'inline']\n",
      "INFO:tensorflow:Loss: 16.488373 | local step: 123 | global step: 2401 | Output: [b'nuts']\n",
      "INFO:tensorflow:Loss: 33.68955 | local step: 80 | global step: 2501 | Output: [b'managed']\n",
      "INFO:tensorflow:Loss: 41.768215 | local step: 8 | global step: 2601 | Output: [b'haply']\n",
      "INFO:tensorflow:Loss: 38.242733 | local step: 108 | global step: 2701 | Output: [b'bliss']\n",
      "INFO:tensorflow:Loss: 32.197697 | local step: 48 | global step: 2801 | Output: [b'customer']\n",
      "INFO:tensorflow:Loss: 28.96615 | local step: 148 | global step: 2901 | Output: [b'dignity']\n",
      "INFO:tensorflow:Loss: 25.449121 | local step: 85 | global step: 3001 | Output: [b'sworn']\n",
      "INFO:tensorflow:Loss: 21.872309 | local step: 17 | global step: 3101 | Output: [b'apt']\n",
      "INFO:tensorflow:Loss: 32.96243 | local step: 117 | global step: 3201 | Output: [b'spirits']\n",
      "INFO:tensorflow:Loss: 31.08154 | local step: 53 | global step: 3301 | Output: [b'seller']\n",
      "INFO:tensorflow:Loss: 20.95673 | local step: 153 | global step: 3401 | Output: [b'source']\n",
      "INFO:tensorflow:Loss: 20.12601 | local step: 95 | global step: 3501 | Output: [b'mon']\n",
      "INFO:tensorflow:Loss: 39.008392 | local step: 35 | global step: 3601 | Output: [b'jungle']\n",
      "INFO:tensorflow:Loss: 25.094423 | local step: 135 | global step: 3701 | Output: [b'une']\n",
      "INFO:tensorflow:Loss: 33.37661 | local step: 71 | global step: 3801 | Output: [b'displayed']\n",
      "INFO:tensorflow:Loss: 23.093027 | local step: 24 | global step: 3901 | Output: [b'films']\n",
      "INFO:tensorflow:Loss: 25.247898 | local step: 124 | global step: 4001 | Output: [b'engineer']\n",
      "INFO:tensorflow:Loss: 24.711868 | local step: 71 | global step: 4101 | Output: [b'ricky']\n",
      "INFO:tensorflow:Loss: 25.020332 | local step: 14 | global step: 4201 | Output: [b'gaudy']\n",
      "INFO:tensorflow:Loss: 31.280611 | local step: 114 | global step: 4301 | Output: [b'gazeth']\n",
      "INFO:tensorflow:Loss: 22.385805 | local step: 50 | global step: 4401 | Output: [b'rats']\n",
      "INFO:tensorflow:Loss: 9.506249 | local step: 150 | global step: 4501 | Output: [b'periodically']\n",
      "INFO:tensorflow:Loss: 29.309523 | local step: 89 | global step: 4601 | Output: [b'salt']\n",
      "INFO:tensorflow:Loss: 8.774947 | local step: 30 | global step: 4701 | Output: [b'salvador']\n",
      "INFO:tensorflow:Loss: 30.790064 | local step: 130 | global step: 4801 | Output: [b'percentage']\n",
      "INFO:tensorflow:Loss: 22.090017 | local step: 75 | global step: 4901 | Output: [b'eu']\n",
      "INFO:tensorflow:Loss: 41.912334 | local step: 19 | global step: 5001 | Output: [b'associates']\n",
      "INFO:tensorflow:Loss: 29.209902 | local step: 119 | global step: 5101 | Output: [b'manner']\n",
      "INFO:tensorflow:Loss: 22.535248 | local step: 78 | global step: 5201 | Output: [b'oxide']\n",
      "INFO:tensorflow:Loss: 35.651062 | local step: 20 | global step: 5301 | Output: [b'mp']\n",
      "INFO:tensorflow:Loss: 25.880266 | local step: 120 | global step: 5401 | Output: [b'indianapolis']\n",
      "INFO:tensorflow:Loss: 34.450394 | local step: 77 | global step: 5501 | Output: [b'lottery']\n",
      "INFO:tensorflow:Loss: 20.237932 | local step: 30 | global step: 5601 | Output: [b'ornaments']\n",
      "INFO:tensorflow:Loss: 13.625372 | local step: 130 | global step: 5701 | Output: [b'sam']\n",
      "INFO:tensorflow:Loss: 40.64825 | local step: 71 | global step: 5801 | Output: [b'sacred']\n",
      "INFO:tensorflow:Loss: 25.71102 | local step: 24 | global step: 5901 | Output: [b'teach']\n",
      "INFO:tensorflow:Loss: 16.048485 | local step: 124 | global step: 6001 | Output: [b'coating']\n",
      "INFO:tensorflow:Loss: 24.961117 | local step: 72 | global step: 6101 | Output: [b'user']\n",
      "INFO:tensorflow:Loss: 16.656961 | local step: 7 | global step: 6201 | Output: [b'dying']\n",
      "INFO:tensorflow:Loss: 33.249947 | local step: 107 | global step: 6301 | Output: [b'xxxnewlinexxx']\n",
      "INFO:tensorflow:Loss: 38.956806 | local step: 59 | global step: 6401 | Output: [b'dear']\n",
      "INFO:tensorflow:Loss: 20.677755 | local step: 159 | global step: 6501 | Output: [b'sync']\n",
      "INFO:tensorflow:Loss: 32.083637 | local step: 90 | global step: 6601 | Output: [b'representing']\n",
      "INFO:tensorflow:Loss: 27.06526 | local step: 36 | global step: 6701 | Output: [b'fighters']\n",
      "INFO:tensorflow:Loss: 43.272957 | local step: 136 | global step: 6801 | Output: [b'willing']\n",
      "INFO:tensorflow:Loss: 29.033955 | local step: 83 | global step: 6901 | Output: [b'pretty']\n",
      "INFO:tensorflow:Loss: 23.344471 | local step: 31 | global step: 7001 | Output: [b'deeds']\n",
      "INFO:tensorflow:Loss: 13.68156 | local step: 131 | global step: 7101 | Output: [b'unusual']\n",
      "INFO:tensorflow:Loss: 43.093025 | local step: 72 | global step: 7201 | Output: [b'antibodies']\n",
      "INFO:tensorflow:Loss: 13.209044 | local step: 7 | global step: 7301 | Output: [b'romance']\n",
      "INFO:tensorflow:Loss: 14.269391 | local step: 107 | global step: 7401 | Output: [b'reveals']\n",
      "INFO:tensorflow:Loss: 24.8192 | local step: 55 | global step: 7501 | Output: [b'portsmouth']\n",
      "INFO:tensorflow:Loss: 18.287182 | local step: 6 | global step: 7601 | Output: [b'xxxnewlinexxx']\n",
      "INFO:tensorflow:Loss: 23.441244 | local step: 106 | global step: 7701 | Output: [b'report']\n",
      "INFO:tensorflow:Loss: 22.226427 | local step: 51 | global step: 7801 | Output: [b'column']\n",
      "INFO:tensorflow:Loss: 20.512823 | local step: 151 | global step: 7901 | Output: [b'documented']\n",
      "INFO:tensorflow:Loss: 32.560616 | local step: 83 | global step: 8001 | Output: [b'ion']\n",
      "INFO:tensorflow:Loss: 36.479855 | local step: 36 | global step: 8101 | Output: [b'betty']\n",
      "INFO:tensorflow:Loss: 44.313225 | local step: 136 | global step: 8201 | Output: [b'project']\n",
      "INFO:tensorflow:Loss: 5.152681 | local step: 93 | global step: 8301 | Output: [b'?']\n",
      "INFO:tensorflow:Loss: 25.846916 | local step: 44 | global step: 8401 | Output: [b'evaluated']\n",
      "INFO:tensorflow:Loss: 15.011711 | local step: 1 | global step: 8501 | Output: [b'xxxnewlinexxx']\n",
      "INFO:tensorflow:Loss: 25.926245 | local step: 101 | global step: 8601 | Output: [b'scroll']\n",
      "INFO:tensorflow:Loss: 23.811367 | local step: 42 | global step: 8701 | Output: [b'demanding']\n",
      "INFO:tensorflow:Loss: 11.979412 | local step: 142 | global step: 8801 | Output: [b'slim']\n",
      "INFO:tensorflow:Loss: 26.864353 | local step: 85 | global step: 8901 | Output: [b'pre']\n",
      "INFO:tensorflow:Loss: 25.543756 | local step: 36 | global step: 9001 | Output: [b'stereo']\n",
      "INFO:tensorflow:Loss: 40.587723 | local step: 136 | global step: 9101 | Output: [b'musicians']\n",
      "INFO:tensorflow:Loss: 19.639679 | local step: 92 | global step: 9201 | Output: [b'and']\n",
      "INFO:tensorflow:Loss: 20.07402 | local step: 47 | global step: 9301 | Output: [b'ws']\n",
      "INFO:tensorflow:Loss: 23.427437 | local step: 147 | global step: 9401 | Output: [b'saw']\n",
      "INFO:tensorflow:Loss: 27.178427 | local step: 92 | global step: 9501 | Output: [b'syndicate']\n",
      "INFO:tensorflow:Loss: 21.079567 | local step: 43 | global step: 9601 | Output: [b'wv']\n",
      "INFO:tensorflow:Loss: 33.301975 | local step: 143 | global step: 9701 | Output: [b'peas']\n",
      "INFO:tensorflow:Loss: 21.52123 | local step: 85 | global step: 9801 | Output: [b',']\n",
      "INFO:tensorflow:Loss: 30.739727 | local step: 36 | global step: 9901 | Output: [b'restoration']\n",
      "INFO:tensorflow:Loss: 10.767063 | local step: 136 | global step: 10001 | Output: [b'periodically']\n",
      "INFO:tensorflow:Loss: 19.557365 | local step: 81 | global step: 10101 | Output: [b'be']\n",
      "INFO:tensorflow:Loss: 20.00339 | local step: 44 | global step: 10201 | Output: [b'completing']\n",
      "INFO:tensorflow:Loss: 17.028004 | local step: 4 | global step: 10301 | Output: [b',']\n",
      "INFO:tensorflow:Loss: 22.58449 | local step: 104 | global step: 10401 | Output: [b'toil']\n",
      "INFO:tensorflow:Loss: 21.558516 | local step: 63 | global step: 10501 | Output: [b\"'\"]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # Graph\n",
    "    # Dataset\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim50/1\")\n",
    "    string2idtable = tf.contrib.lookup.index_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\", num_oov_buckets=0)\n",
    "    id2stringtabel = tf.contrib.lookup.index_to_string_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\")\n",
    "    def _insertSpace(sentence):\n",
    "        sentence = sentence.decode()\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'([\\W\\d])', r' \\1 ', sentence)\n",
    "        return sentence\n",
    "    # Use tf.string_split if want to split string\n",
    "    # def _split(sentence):\n",
    "    #    return sentence.split()\n",
    "    def _getLabel(sentence):\n",
    "        splited = tf.string_split([sentence]).values\n",
    "        sentence = splited\n",
    "        # Use \" \" as x_0\n",
    "        # sentence = tf.concat([tf.constant([\" \"], dtype=tf.string), sentence[0:-1]], 0)\n",
    "        sentence = sentence[0:-1]\n",
    "        ids = string2idtable.lookup(splited)\n",
    "        ids = tf.one_hot(ids, 11405)\n",
    "        return sentence, ids# {\"sentence\":sentence}, ids\n",
    "    filenames = [\"poems/shakespeare/sonnets.txt\"]\n",
    "    # TODO Use skip and filter methods to preprocess data rather than manually do it\n",
    "    # TODO Use Dataset.map method to map '\\n' to 'xxxnewlinexxx'\n",
    "    dataset = tf.data.TextLineDataset(filenames)\n",
    "    dataset = dataset.map(lambda sentence: tf.py_func(_insertSpace, [sentence], tf.string))\n",
    "    dataset = dataset.map(_getLabel)\n",
    "    # dataset = dataset.shuffle(buffer_size=10000)\n",
    "    # dataset = dataset.batch(4)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_poem = iterator.get_next()\n",
    "\n",
    "    # Variables\n",
    "    \"\"\"\n",
    "    def get_reuse_var(name, shape, scope=\"dense\", initializer=tf.random_normal_initializer):\n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "            var = tf.get_variable(name, shape=shape, initializer=initializer)\n",
    "        return var\n",
    "    softmax_w = get_reuse_var(\"softmax_w\", shape=[400, 11405])\n",
    "    softmax_b = get_reuse_var(\"softmax_b\", shape=[1, 11405])\n",
    "    \"\"\"\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", shape=[400, 11405], initializer=tf.random_normal_initializer)\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", shape=[1, 11405], initializer=tf.random_normal_initializer)\n",
    "\n",
    "    # TODO Use tuple for state\n",
    "    lstm = tf.contrib.rnn.LSTMCell(400, state_is_tuple=False, initializer=tf.random_normal_initializer, reuse=tf.AUTO_REUSE, name=\"LSTM1\")\n",
    "\n",
    "    # Model\n",
    "    sentence, label = next_poem\n",
    "    sentence = embed(sentence)\n",
    "    # sentence = tf.concat([tf.zeros([1, 50]), sentence[1:]], 0)\n",
    "    sentence = tf.concat([tf.zeros([1, 50]), sentence], 0, name=\"Insert_X_0\")\n",
    "\n",
    "\n",
    "    state = tf.placeholder(shape=[1, lstm.state_size], dtype=tf.float32, name=\"Previous_State\")\n",
    "    # TODO All loop must move to session??\n",
    "    # for _ in range(total_steps):\n",
    "    #x, y = sentence, label\n",
    "\n",
    "    x = tf.placeholder(shape=[50], dtype=tf.float32, name=\"Input_Word\")\n",
    "    y = tf.placeholder(shape=[11405], dtype=tf.int32, name=\"Target_Word\")\n",
    "    # TODO How to Access timestep?\n",
    "    input_word = tf.reshape(x, [1, 50])\n",
    "    output, out_state = lstm(input_word, state)\n",
    "    logits = tf.add(tf.matmul(output, softmax_w), softmax_b)\n",
    "    possibility = tf.nn.softmax(logits=logits, name=\"Possibilities\")\n",
    "    word_index = tf.argmax(possibility, axis=-1, name=\"Predict_Word_index\")\n",
    "    word = id2stringtabel.lookup(word_index)\n",
    "    loss_op = tf.losses.softmax_cross_entropy(onehot_labels=[y], logits=logits)\n",
    "    tf.summary.scalar(name=\"loss\", tensor=loss_op)\n",
    "    merged = tf.summary.merge_all()\n",
    "    # loss_op = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits, name=\"Loss\")\n",
    "\n",
    "    #total_loss = tf.get_variable(shape=[], dtype=tf.float32, name=\"Total_Cost\", initializer=tf.zeros_initializer)\n",
    "\n",
    "    # TODO Train Ops\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "    # sess = tf.Session()\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(\"tmp/log/\", sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tf.tables_initializer().run(session=sess)\n",
    "        sess.run(iterator.initializer)\n",
    "        total_steps = 1000\n",
    "        # total_loss = 0.0\n",
    "        # TODO All loop must move to session??\n",
    "        global_step = 1\n",
    "        for _ in range(total_steps):\n",
    "            wordsVec, targets = sess.run([sentence, label])\n",
    "            # TODO How to Access timestep?\n",
    "            # Initial\n",
    "            _, pre_state = sess.run([output, out_state], feed_dict={x: wordsVec[0], y: targets[0], state: np.zeros([1, lstm.state_size], dtype=float)})\n",
    "            for i in range(1, wordsVec.shape[0]):\n",
    "                # total_loss += loss_op\n",
    "                _, pre_state, loss, summary, out = sess.run([train_op, out_state, loss_op, merged, word], feed_dict={x: wordsVec[i], y: targets[i], state: pre_state})\n",
    "                writer.add_summary(summary, global_step=global_step)\n",
    "                tf.logging.log_every_n(tf.logging.INFO, \"Loss: %s | local step: %s | global step: %s | Output: %s\", 100, loss, i, global_step, out)\n",
    "                global_step += 1\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "\n",
    "#for _ in range(1):\n",
    "#    poem = sess.run(next_poem)\n",
    "#    print(type(poem[0][\"sentence\"]))\n",
    "#    print(type(poem[1]))\n",
    "# (lambda: next_poem)() # As input_fn? for Estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'lab'\n",
      "b'#'\n",
      "b'#'\n",
      "b'#'\n",
      "b'#'\n",
      "b'#'\n",
      "b'#'\n",
      "b'#'\n",
      "b'#'\n",
      "b'#'\n",
      "b'#'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idice = id2stringtabel.lookup(tf.constant([random.randint(0, 11404)], dtype=tf.int64))\n",
    "idix, prev_state = sess.run([idice, out_state], feed_dict={x: np.zeros([50]), state: np.zeros([1, lstm.state_size], dtype=float)})\n",
    "idix = str(idix[0])\n",
    "print(idix)\n",
    "w = sess.run(tf.reshape(embed([idix]), shape=[50]))\n",
    "idice = id2stringtabel.lookup(tf.argmax(word))\n",
    "for i in range(10):\n",
    "    idix, prev_state = sess.run([idice, out_state], feed_dict={x:w, state: prev_state})\n",
    "    w = str(idix)\n",
    "    print(w)\n",
    "    w = sess.run(tf.reshape(embed([w]), shape=[50]))\n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # Graph\n",
    "    # Dataset\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim50/1\")\n",
    "    string2idtable = tf.contrib.lookup.index_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\", num_oov_buckets=0)\n",
    "    id2stringtabel = tf.contrib.lookup.index_to_string_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\")\n",
    "    def _insertSpace(sentence):\n",
    "        sentence = sentence.decode()\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'([\\W\\d])', r' \\1 ', sentence)\n",
    "        return sentence\n",
    "    # Use tf.string_split if want to split string\n",
    "    # def _split(sentence):\n",
    "    #    return sentence.split()\n",
    "    def _getLabel(sentence):\n",
    "        splited = tf.string_split([sentence]).values\n",
    "        sentence = splited\n",
    "        # Use \" \" as x_0\n",
    "        # sentence = tf.concat([tf.constant([\" \"], dtype=tf.string), sentence[0:-1]], 0)\n",
    "        sentence = sentence[0:-1]\n",
    "        ids = string2idtable.lookup(splited)\n",
    "        ids = tf.one_hot(ids, 11405)\n",
    "        return sentence, ids# {\"sentence\":sentence}, ids\n",
    "    filenames = [\"poems/shakespeare/sonnets.txt\"]\n",
    "    # TODO Use skip and filter methods to preprocess data rather than manually do it\n",
    "    # TODO Use Dataset.map method to map '\\n' to 'xxxnewlinexxx'\n",
    "    dataset = tf.data.TextLineDataset(filenames)\n",
    "    dataset = dataset.map(lambda sentence: tf.py_func(_insertSpace, [sentence], tf.string))\n",
    "    dataset = dataset.map(_getLabel)\n",
    "    # dataset = dataset.shuffle(buffer_size=10000)\n",
    "    # dataset = dataset.batch(4)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_poem = iterator.get_next()\n",
    "    \n",
    "    # Variables\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", shape=[128, 11405], initializer=tf.random_normal_initializer)\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", shape=[1, 11405], initializer=tf.random_normal_initializer)\n",
    "\n",
    "    # TODO Use tuple for state\n",
    "    lstm = tf.contrib.rnn.LSTMCell(128, state_is_tuple=False, initializer=tf.random_normal_initializer, reuse=tf.AUTO_REUSE, name=\"lstm1\")\n",
    "    \n",
    "    # Model\n",
    "    sentence, label = next_poem\n",
    "    sentence = embed(sentence)\n",
    "    # sentence = tf.concat([tf.zeros([1, 50]), sentence[1:]], 0)\n",
    "    sentence = tf.concat([tf.zeros([1, 50]), sentence], 0)\n",
    "\n",
    "    # TODO Train Ops\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tf.tables_initializer().run(session=sess)\n",
    "        sess.run(iterator.initializer)\n",
    "        \n",
    "        pre_state = tf.placeholder(shape=[1, lstm.state_size], dtype=tf.float32)\n",
    "        state = pre_state\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        total_steps = 1\n",
    "        # TODO All loop must move to session??\n",
    "        for _ in range(total_steps):\n",
    "            x, y = sess.run([sentence, label])\n",
    "            # TODO How to Access timestep?\n",
    "            for i in range(x.shape[0]):\n",
    "                input_word = tf.reshape(x[i], [1, 50])\n",
    "                output, state = lstm(input_word, state)\n",
    "                logits = tf.add(tf.matmul(output, softmax_w), softmax_b)\n",
    "                possibility = tf.nn.softmax(logits=logits, name=\"Possibilities\")\n",
    "                word = tf.argmax(possibility, axis=-1, name=\"Predict_Word_index\")\n",
    "                loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y[i], logits=logits, name=\"Loss\")\n",
    "                total_loss += loss\n",
    "                # tf.logging.log_every_n(tf.logging.INFO, \"%s\", 1, sess.run(loss))\n",
    "        \n",
    "        \n",
    "        writer = tf.summary.FileWriter(\"tmp/log/\", sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tf.tables_initializer().run(session=sess)\n",
    "        sess.run(iterator.initializer)\n",
    "        print(sess.run(word, feed_dict={pre_state: np.zeros([1,256], dtype=float)}))\n",
    "        writer.close()\n",
    "            \n",
    "            \n",
    "            \n",
    "#for _ in range(1):\n",
    "#    poem = sess.run(next_poem)\n",
    "#    print(type(poem[0][\"sentence\"]))\n",
    "#    print(type(poem[1]))\n",
    "# (lambda: next_poem)() # As input_fn? for Estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
