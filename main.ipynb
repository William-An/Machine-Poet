{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange\n",
    "from tqdm import tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v1 - Current\n",
    "* One layer of LSTM\n",
    "* Non-batch train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim50/1\")\n",
    "string2idtable = tf.contrib.lookup.index_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\", num_oov_buckets=0)\n",
    "id2stringtabel = tf.contrib.lookup.index_to_string_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\")\n",
    "def _insertSpace(sentence):\n",
    "    sentence = sentence.decode()\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'([\\W\\d])', r' \\1 ', sentence)\n",
    "    return sentence\n",
    "# Use tf.string_split if want to split string\n",
    "# def _split(sentence):\n",
    "#    return sentence.split()\n",
    "def _getLabel(sentence):\n",
    "    splited = tf.string_split([sentence]).values\n",
    "    sentence = splited\n",
    "    # Use \" \" as x_0\n",
    "    # sentence = tf.concat([tf.constant([\" \"], dtype=tf.string), sentence[0:-1]], 0)\n",
    "    sentence = sentence[0:-1]\n",
    "    ids = string2idtable.lookup(splited)\n",
    "    ids = tf.one_hot(ids, 11405)\n",
    "    return sentence, ids# {\"sentence\":sentence}, ids\n",
    "filenames = [\"poems/shakespeare/sonnets.txt\"]\n",
    "# TODO Use skip and filter methods to preprocess data rather than manually do it\n",
    "# TODO Use Dataset.map method to map '\\n' to 'xxxnewlinexxx'\n",
    "dataset = tf.data.TextLineDataset(filenames)\n",
    "dataset = dataset.map(lambda sentence: tf.py_func(_insertSpace, [sentence], tf.string))\n",
    "dataset = dataset.map(_getLabel)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "# dataset = dataset.batch(4)\n",
    "dataset = dataset.repeat()\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_poem = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization and Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Graph().as_default():\n",
    "# Graph\n",
    "# Variables\n",
    "\"\"\"\n",
    "def get_reuse_var(name, shape, scope=\"dense\", initializer=tf.random_normal_initializer):\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        var = tf.get_variable(name, shape=shape, initializer=initializer)\n",
    "    return var\n",
    "softmax_w = get_reuse_var(\"softmax_w\", shape=[400, 11405])\n",
    "softmax_b = get_reuse_var(\"softmax_b\", shape=[1, 11405])\n",
    "\"\"\"\n",
    "softmax_w = tf.get_variable(\"softmax_w\", shape=[400, 11405], initializer=tf.random_normal_initializer)\n",
    "softmax_b = tf.get_variable(\"softmax_b\", shape=[1, 11405], initializer=tf.random_normal_initializer)\n",
    "\n",
    "# TODO Use tuple for state\n",
    "lstm = tf.contrib.rnn.LSTMCell(400, state_is_tuple=False, initializer=tf.random_normal_initializer, reuse=tf.AUTO_REUSE, name=\"LSTM1\")\n",
    "\n",
    "# Model\n",
    "sentence, label = next_poem\n",
    "sentence = embed(sentence)\n",
    "sentence = tf.concat([tf.zeros([1, 50]), sentence], 0, name=\"Insert_X_0\")\n",
    "\n",
    "\n",
    "state = tf.placeholder(shape=[1, lstm.state_size], dtype=tf.float32, name=\"Previous_State\")\n",
    "\n",
    "x = tf.placeholder(shape=[50], dtype=tf.float32, name=\"Input_Word\")\n",
    "y = tf.placeholder(shape=[11405], dtype=tf.int32, name=\"Target_Word\")\n",
    "# TODO How to Access timestep?\n",
    "input_word = tf.reshape(x, [1, 50])\n",
    "output, out_state = lstm(input_word, state)\n",
    "logits = tf.add(tf.matmul(output, softmax_w), softmax_b)\n",
    "possibility = tf.nn.softmax(logits=logits, name=\"Possibilities\")\n",
    "word_index = tf.argmax(possibility, axis=-1, name=\"Predict_Word_index\")\n",
    "word = id2stringtabel.lookup(word_index)\n",
    "loss_op = tf.losses.softmax_cross_entropy(onehot_labels=[y], logits=logits)\n",
    "tf.summary.scalar(name=\"loss\", tensor=loss_op)\n",
    "merged = tf.summary.merge_all()\n",
    "# loss_op = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits, name=\"Loss\")\n",
    "\n",
    "#total_loss = tf.get_variable(shape=[], dtype=tf.float32, name=\"Total_Cost\", initializer=tf.zeros_initializer)\n",
    "\n",
    "# TODO Train Ops\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "sess = tf.Session()\n",
    "# with tf.Session() as sess:\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.tables_initializer().run(session=sess)\n",
    "sess.run(iterator.initializer)\n",
    "\n",
    "# Trainning log\n",
    "writer = tf.summary.FileWriter(\"tmp/log/\", sess.graph)\n",
    "global_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_poems = 1000\n",
    "for _ in range(total_poems):\n",
    "    wordsVec, targets = sess.run([sentence, label])\n",
    "    # TODO How to Access timestep?\n",
    "    # Initial\n",
    "    _, pre_state = sess.run([output, out_state], feed_dict={x: wordsVec[0], y: targets[0], state: np.zeros([1, lstm.state_size], dtype=float)})\n",
    "    for i in range(1, wordsVec.shape[0]):\n",
    "        # total_loss += loss_op\n",
    "        # print(wordsVec[i].shape)\n",
    "        _, pre_state, loss, summary, out = sess.run([train_op, out_state, loss_op, merged, word], feed_dict={x: wordsVec[i], y: targets[i], state: pre_state})\n",
    "        writer.add_summary(summary, global_step=global_step)\n",
    "        tf.logging.log_every_n(tf.logging.INFO, \"Loss: %s | local step: %s | global step: %s | Output: %s\", 100, loss, i, global_step, out)\n",
    "        global_step += 1                             \n",
    "print(\"Trainning poems: \", total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(input_str):\n",
    "    return re.sub(\"xxxnewlinexxx\", \"\\n\", input_str)\n",
    "random_word = id2stringtabel.lookup(tf.constant([random.randint(0, 11404)], dtype=tf.int64))\n",
    "word_predict, prev_state = sess.run([random_word, out_state], feed_dict={x: np.zeros([50]), state: np.zeros([1, lstm.state_size], dtype=float)})\n",
    "print(word_predict)\n",
    "poem = [word_predict[0].decode()]\n",
    "while word_predict[0].decode() != \"xxxendxxx\":\n",
    "    word_predict = sess.run(tf.reshape(embed(word_predict), shape=[50]))\n",
    "    word_predict, prev_state = sess.run([word, out_state], feed_dict={x:word_predict, state: prev_state})\n",
    "    poem.append(word_predict[0].decode())\n",
    "    print(pretty(\" \".join(poem)))\n",
    "# Dump to generated dir\n",
    "generated_poem = pretty(\" \".join(poem))\n",
    "filename = \"-\".join(poem[:5])\n",
    "with open(\"generated/\"+filename+\".txt\", \"w\") as f:\n",
    "    f.write(generated_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write graph to file\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 - Multi-RNN with Batch\n",
    "* Deep LSTM\n",
    "    - Try Different number of layers\n",
    "    - Try Different LSTM units amount\n",
    "* Batch\n",
    "* Try other Embedding modules\n",
    "    - https://tfhub.dev/google/nnlm-en-dim50/1\n",
    "    - https://tfhub.dev/google/nnlm-en-dim50-with-normalization/1\n",
    "    - https://tfhub.dev/google/nnlm-en-dim128/1\n",
    "    - https://tfhub.dev/google/nnlm-en-dim128-with-normalization/1\n",
    "    - https://tfhub.dev/google/Wiki-words-250/1\n",
    "    - https://tfhub.dev/google/Wiki-words-250-with-normalization/1\n",
    "    - https://tfhub.dev/google/Wiki-words-500/1\n",
    "    - https://tfhub.dev/google/Wiki-words-500-with-normalization/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
