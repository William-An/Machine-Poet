{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v1 - Current\n",
    "* One layer of LSTM\n",
    "* Non-batch train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim50/1\")\n",
    "string2idtable = tf.contrib.lookup.index_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\", num_oov_buckets=0)\n",
    "id2stringtabel = tf.contrib.lookup.index_to_string_table_from_file(vocabulary_file=\"vocabulary-shakespeare.txt\")\n",
    "def _insertSpace(sentence):\n",
    "    sentence = sentence.decode()\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'([\\W\\d])', r' \\1 ', sentence)\n",
    "    return sentence\n",
    "def _getLabel(sentence):\n",
    "    splited = tf.string_split([sentence]).values\n",
    "    sentence = splited\n",
    "    sentence = sentence[0:-1]\n",
    "    ids = string2idtable.lookup(splited)\n",
    "    ids = tf.one_hot(ids, 11405)\n",
    "    return sentence, ids\n",
    "filenames = [\"poems/shakespeare/sonnets.txt\"]\n",
    "dataset = tf.data.TextLineDataset(filenames)\n",
    "dataset = dataset.map(lambda sentence: tf.py_func(_insertSpace, [sentence], tf.string))\n",
    "dataset = dataset.map(_getLabel)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "# dataset = dataset.batch(4)\n",
    "dataset = dataset.repeat()\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_poem = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization and Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Graph().as_default():\n",
    "# Graph\n",
    "# Variables\n",
    "softmax_w = tf.get_variable(\"softmax_w\", shape=[400, 11405], initializer=tf.random_normal_initializer)\n",
    "softmax_b = tf.get_variable(\"softmax_b\", shape=[1, 11405], initializer=tf.random_normal_initializer)\n",
    "\n",
    "# TODO Use tuple for state\n",
    "lstm = tf.contrib.rnn.LSTMCell(400, state_is_tuple=False, initializer=tf.random_normal_initializer, reuse=tf.AUTO_REUSE, name=\"LSTM1\")\n",
    "\n",
    "# Model\n",
    "sentence, label = next_poem\n",
    "sentence = embed(sentence)\n",
    "sentence = tf.concat([tf.zeros([1, 50]), sentence], 0, name=\"Insert_X_0\")\n",
    "\n",
    "state = tf.placeholder(shape=[1, lstm.state_size], dtype=tf.float32, name=\"Previous_State\")\n",
    "\n",
    "# Input and Label\n",
    "x = tf.placeholder(shape=[1, 50], dtype=tf.float32, name=\"Input_Word\")\n",
    "y = tf.placeholder(shape=[1, 11405], dtype=tf.int32, name=\"Target_Word\")\n",
    "\n",
    "input_word = tf.reshape(x, [1, 50])\n",
    "output, out_state = lstm(input_word, state)\n",
    "logits = tf.add(tf.matmul(output, softmax_w), softmax_b)\n",
    "possibility = tf.nn.softmax(logits=logits, name=\"Possibilities\")\n",
    "word_index = tf.argmax(possibility, axis=-1, name=\"Predict_Word_index\")\n",
    "word = id2stringtabel.lookup(word_index)\n",
    "loss_op = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits)\n",
    "tf.summary.scalar(name=\"loss\", tensor=loss_op)\n",
    "merged = tf.summary.merge_all()\n",
    "# loss_op = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits, name=\"Loss\")\n",
    "\n",
    "#total_loss = tf.get_variable(shape=[], dtype=tf.float32, name=\"Total_Cost\", initializer=tf.zeros_initializer)\n",
    "\n",
    "# Train Ops\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Global step as variable for persistent tensorboard output\n",
    "global_step = tf.get_variable(\"Global_Steps\", [], initializer=tf.ones_initializer, dtype=tf.int64)\n",
    "\n",
    "# Variables saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session()\n",
    "# with tf.Session() as sess:\n",
    "try:\n",
    "    saver.restore(sess, \"tmp/model/model.ckpt\")\n",
    "except tf.errors.NotFoundError:\n",
    "    pass\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.tables_initializer().run(session=sess)\n",
    "sess.run(iterator.initializer)\n",
    "\n",
    "# Trainning log\n",
    "# TODO writer = tf.summary.FileWriter(\"tmp/log/\"+Modelv1+embedding_version, sess.graph)\n",
    "writer = tf.summary.FileWriter(\"tmp/log/\", sess.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_poems = 10000\n",
    "for _ in range(total_poems):\n",
    "    wordsVec, targets = sess.run([sentence, label])\n",
    "    # TODO How to Access timestep?\n",
    "    # Initial\n",
    "    _, pre_state = sess.run([output, out_state], feed_dict={x: [wordsVec[0]], y: [targets[0]], state: np.zeros([1, lstm.state_size], dtype=float)})\n",
    "    for i in range(1, wordsVec.shape[0]):\n",
    "        # total_loss += loss_op\n",
    "        # print(wordsVec[i].shape)\n",
    "        g_steps = global_step.eval(session=sess)\n",
    "        _, pre_state, loss, summary, out = sess.run([train_op, out_state, loss_op, merged, word], feed_dict={x: [wordsVec[i]], y: [targets[i]], state: pre_state})\n",
    "        writer.add_summary(summary, global_step=g_steps)\n",
    "        # TODO Should Average loss\n",
    "        tf.logging.log_every_n(tf.logging.INFO, \"Loss: %s | local step: %s | global step: %s | Output: %s\", 100, loss, i, g_steps, out)\n",
    "        if g_steps%500 == 0:\n",
    "            # Store Variables Every 10000 times\n",
    "            saver.save(sess, \"tmp/model/model.ckpt\")\n",
    "        global_step += 1                             \n",
    "print(\"Trainning poems: \", total_poems)\n",
    "saver.save(sess, \"tmp/model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(input_str):\n",
    "    return re.sub(\"xxxnewlinexxx\", \"\\n\", input_str)\n",
    "random_word = id2stringtabel.lookup(tf.constant([random.randint(0, 11404)], dtype=tf.int64))\n",
    "word_predict, prev_state = sess.run([random_word, out_state], feed_dict={x: np.zeros([1, 50]), state: np.zeros([1, lstm.state_size], dtype=float)})\n",
    "print(word_predict)\n",
    "poem = [word_predict[0].decode()]\n",
    "while word_predict[0].decode() != \"xxxendxxx\":\n",
    "    word_predict = sess.run(embed(word_predict))\n",
    "    word_predict, prev_state = sess.run([word, out_state], feed_dict={x:word_predict, state: prev_state})\n",
    "    poem.append(word_predict[0].decode())\n",
    "    print(pretty(\" \".join(poem)))\n",
    "# Dump to generated dir\n",
    "generated_poem = pretty(\" \".join(poem))\n",
    "filename = \"-\".join(poem[:5])\n",
    "with open(\"generated/\"+filename+\".txt\", \"w\") as f:\n",
    "    f.write(generated_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write graph to file\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 - Multi-RNN with Batch\n",
    "* Deep LSTM\n",
    "    - Try Different number of layers\n",
    "    - Try Different LSTM units amount\n",
    "* Batch\n",
    "* Try other Embedding modules\n",
    "    - https://tfhub.dev/google/nnlm-en-dim50/1\n",
    "    - https://tfhub.dev/google/nnlm-en-dim50-with-normalization/1\n",
    "    - https://tfhub.dev/google/nnlm-en-dim128/1\n",
    "    - https://tfhub.dev/google/nnlm-en-dim128-with-normalization/1\n",
    "    - https://tfhub.dev/google/Wiki-words-250/1\n",
    "    - https://tfhub.dev/google/Wiki-words-250-with-normalization/1\n",
    "    - https://tfhub.dev/google/Wiki-words-500/1\n",
    "    - https://tfhub.dev/google/Wiki-words-500-with-normalization/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
